{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur6msxX-itwx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Embedding, LSTM, Dense, Dropout,\n",
        "                                   BatchNormalization, GlobalMaxPooling1D)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ GPU...\")\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(f\"âœ… GPU Ù…ØªØ§Ø­: {physical_devices[0]}\")\n",
        "else:\n",
        "    print(\"ğŸ’» ØªØ´ØºÙŠÙ„ Ø¹Ù„Ù‰ CPU\")\n"
      ],
      "metadata": {
        "id": "7b4mnfkbi_21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "print(\"ğŸ“ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
        "data_train = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"ğŸ“Š Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {data_train.shape}\")\n",
        "print(f\"ğŸ“Š Ø­Ø¬Ù… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {data_test.shape}\")"
      ],
      "metadata": {
        "id": "EQm8rjFGjHVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ğŸ“ˆ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª:\")\n",
        "class_counts = data_train['Class Index'].value_counts().sort_index()\n",
        "print(class_counts)\n"
      ],
      "metadata": {
        "id": "V9qret_1jL7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
        "data_train['Title'] = data_train['Title'].fillna('').astype(str)\n",
        "data_train['Description'] = data_train['Description'].fillna('').astype(str)\n",
        "data_test['Title'] = data_test['Title'].fillna('').astype(str)\n",
        "data_test['Description'] = data_test['Description'].fillna('').astype(str)\n"
      ],
      "metadata": {
        "id": "VwUndxFWjPO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ\n",
        "texts_train = (data_train['Title'] + ' ' + data_train['Description']).tolist()\n",
        "texts_test = (data_test['Title'] + ' ' + data_test['Description']).tolist()\n",
        "\n",
        "print(f\"ğŸ“ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ: {texts_train[0][:100]}...\")"
      ],
      "metadata": {
        "id": "zsj6lpjpjSxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©\n",
        "max_words = 20000  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ø³Ø±Ø¹Ø©\n",
        "max_len = 100      # ØªÙ‚Ù„ÙŠÙ„ Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø© Ù„Ù„Ø³Ø±Ø¹Ø©\n",
        "batch_size = 128   # Ø²ÙŠØ§Ø¯Ø© batch size Ù„Ù„Ø³Ø±Ø¹Ø©\n",
        "\n",
        "print(\"ğŸ”¤ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³...\")\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=max_words,\n",
        "    oov_token=\"<OOV>\",\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        ")\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "\n",
        "print(f\"ğŸ“– Ø­Ø¬Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: {min(len(tokenizer.word_index), max_words)}\")\n"
      ],
      "metadata": {
        "id": "-DFs8vQKjWSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ\n",
        "X_train_seq = tokenizer.texts_to_sequences(texts_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "print(f\"âœ‚ï¸ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {X_train_pad.shape}\")\n"
      ],
      "metadata": {
        "id": "g5O-gnXEjYra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(data_train['Class Index'])\n",
        "y_train_cat = to_categorical(y_train)\n",
        "num_classes = y_train_cat.shape[1]\n",
        "\n",
        "print(f\"ğŸ·ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {num_classes}\")\n",
        "print(f\"ğŸ·ï¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª: {label_encoder.classes_}\")\n"
      ],
      "metadata": {
        "id": "KaU4L1KLjapP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "    X_train_pad, y_train_cat,\n",
        "    test_size=0.15,  # ØªÙ‚Ù„ÙŠÙ„ validation set\n",
        "    random_state=42,\n",
        "    stratify=y_train_cat\n",
        ")\n",
        "\n",
        "print(f\"ğŸ”„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {X_train_split.shape}, Ø§Ù„ØªØ­Ù‚Ù‚: {X_val.shape}\")\n",
        "\n",
        "print(\"ğŸ—ï¸ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¨Ø³Ø·...\")\n"
      ],
      "metadata": {
        "id": "KO5N8w7fjcIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø³Ø· ÙˆØ³Ø±ÙŠØ¹\n",
        "model = Sequential([\n",
        "    # Embedding Ø¨Ø³ÙŠØ·\n",
        "    Embedding(\n",
        "        input_dim=max_words,\n",
        "        output_dim=128,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø¹Ø¯ Ù„Ù„Ø³Ø±Ø¹Ø©\n",
        "        input_length=max_len,\n",
        "        mask_zero=True  # ØªØ¬Ø§Ù‡Ù„ padding\n",
        "    ),"
      ],
      "metadata": {
        "id": "rH0qmU_KjcER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # LSTM ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·\n",
        "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
        "\n",
        "    # Dense layers Ø¨Ø³ÙŠØ·Ø©\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "BlbvonOZjbI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer Ù…Ø­Ø³Ù†\n",
        "optimizer = Adam(\n",
        "    learning_rate=0.01,  # learning rate Ø£Ø¹Ù„Ù‰ Ù„Ù„Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©\n",
        "    clipnorm=1.0\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "QdedrMrWjbfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callbacks Ù…Ø­Ø³Ù†Ø©\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,  # patience Ø£Ù‚Ù„\n",
        "        restore_best_weights=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',  # Ù…Ø±Ø§Ù‚Ø¨Ø© accuracy Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† loss\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...\")"
      ],
      "metadata": {
        "id": "vc7XQADgkR0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹\n",
        "history = model.fit(\n",
        "    X_train_split, y_train_split,\n",
        "    epochs=15,  # epochs Ø£Ù‚Ù„\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "TNip7l5HkVdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "print(\"ğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...\")\n",
        "train_acc = model.evaluate(X_train_split, y_train_split, verbose=0)[1]\n",
        "val_acc = model.evaluate(X_val, y_val, verbose=0)[1]\n",
        "\n",
        "print(f\"ğŸ¯ Ø¯Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_acc:.4f}\")\n",
        "print(f\"ğŸ¯ Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù‚Ù‚: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "f3TSBUZ_kXkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "print(\"ğŸ”® Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª...\")\n",
        "y_pred = model.predict(X_test_pad, batch_size=batch_size, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n"
      ],
      "metadata": {
        "id": "Iv8xrXEHkbbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "print(\"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...\")\n",
        "model.save('models/lstm_simple.h5')\n",
        "\n",
        "import pickle\n",
        "with open('models/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "aKYmQEbykbZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!\")"
      ],
      "metadata": {
        "id": "Pn28rYv9kbXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kG_A91BGkbUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3owBM1RkbS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}