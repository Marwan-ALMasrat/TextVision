import streamlit as st
import pandas as pd
import json
import os
from collections import Counter
import re

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ØªØ­Ù…ÙŠÙ„ CSS Ù…Ø®ØµØµ
def load_css():
    st.markdown("""
    <style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1E88E5;
        margin-bottom: 2rem;
    }
    
    .feature-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 5px solid #1E88E5;
    }
    
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        text-align: center;
    }
    
    .success-box {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
    }
    
    .warning-box {
        background-color: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #ffeaa7;
    }
    </style>
    """, unsafe_allow_html=True)

load_css()

# Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ·Ø© (Ù…Ø­Ù„ÙŠØ©)
def simple_text_analysis(text):
    """ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø¨Ø³ÙŠØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Python Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    if not text:
        return {}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    words = text.split()
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    # Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    word_count = len(words)
    char_count = len(text)
    sentence_count = len(sentences)
    
    # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
    word_freq = Counter(words)
    most_common = word_freq.most_common(10)
    
    # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ø´Ø§Ø¹Ø± (ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©)
    positive_words = ['Ø¬ÙŠØ¯', 'Ù…Ù…ØªØ§Ø²', 'Ø±Ø§Ø¦Ø¹', 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø³Ø¹ÙŠØ¯', 'Ù†Ø¬Ø­', 'ØªÙ‚Ø¯Ù…', 'ÙÙˆØ²']
    negative_words = ['Ø³ÙŠØ¡', 'Ø³Ù„Ø¨ÙŠ', 'ÙØ´Ù„', 'Ù…Ø´ÙƒÙ„Ø©', 'Ø®Ø·Ø£', 'Ø­Ø²ÙŠÙ†', 'ØµØ¹Ø¨', 'Ø®Ø³Ø§Ø±Ø©']
    
    positive_count = sum(1 for word in words if word in positive_words)
    negative_count = sum(1 for word in words if word in negative_words)
    
    sentiment = "Ù…Ø­Ø§ÙŠØ¯"
    if positive_count > negative_count:
        sentiment = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif negative_count > positive_count:
        sentiment = "Ø³Ù„Ø¨ÙŠ"
    
    # ØªØµÙ†ÙŠÙ Ø¨Ø³ÙŠØ· Ø­Ø³Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    sports_words = ['ÙƒØ±Ø©', 'Ù…Ø¨Ø§Ø±Ø§Ø©', 'ÙØ±ÙŠÙ‚', 'Ù„Ø§Ø¹Ø¨', 'Ø¨Ø·ÙˆÙ„Ø©', 'Ù…Ù„Ø¹Ø¨']
    politics_words = ['Ø­ÙƒÙˆÙ…Ø©', 'Ø±Ø¦ÙŠØ³', 'ÙˆØ²ÙŠØ±', 'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø³ÙŠØ§Ø³Ø©', 'Ø¨Ø±Ù„Ù…Ø§Ù†']
    tech_words = ['ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'ÙƒÙ…Ø¨ÙŠÙˆØªØ±', 'Ø¥Ù†ØªØ±Ù†Øª', 'ØªØ·Ø¨ÙŠÙ‚', 'Ø¨Ø±Ù†Ø§Ù…Ø¬', 'Ø±Ù‚Ù…ÙŠ']
    
    category_scores = {
        'Ø±ÙŠØ§Ø¶Ø©': sum(1 for word in words if word in sports_words),
        'Ø³ÙŠØ§Ø³Ø©': sum(1 for word in words if word in politics_words),
        'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§': sum(1 for word in words if word in tech_words)
    }
    
    predicted_category = max(category_scores.items(), key=lambda x: x[1])
    if predicted_category[1] == 0:
        predicted_category = ('Ø¹Ø§Ù…', 0)
    
    return {
        'word_count': word_count,
        'char_count': char_count,
        'sentence_count': sentence_count,
        'most_common_words': most_common,
        'sentiment': sentiment,
        'positive_words': positive_count,
        'negative_words': negative_count,
        'category': predicted_category[0],
        'category_confidence': predicted_category[1] / word_count if word_count > 0 else 0
    }

def simple_summarize(text, num_sentences=3):
    """ØªÙ„Ø®ÙŠØµ Ø¨Ø³ÙŠØ· Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø£Ø®Ø° Ø£ÙˆÙ„ ÙˆØ£Ù‡Ù… Ø§Ù„Ø¬Ù…Ù„"""
    if not text:
        return ""
    
    sentences = [s.strip() for s in text.split('.') if s.strip() and len(s) > 20]
    
    if len(sentences) <= num_sentences:
        return '. '.join(sentences) + '.'
    
    # Ø£Ø®Ø° Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ + Ø¬Ù…Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ³Ø·
    summary_sentences = [sentences[0]]  # Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
    
    # Ø¥Ø¶Ø§ÙØ© Ø¬Ù…Ù„ Ù…Ù† Ø§Ù„ÙˆØ³Ø· Ø­Ø³Ø¨ Ø§Ù„Ø·ÙˆÙ„
    if len(sentences) > 3:
        mid_sentences = sentences[1:-1]
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø·ÙˆÙ„ (Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£Ø·ÙˆÙ„ Ø¹Ø§Ø¯Ø© Ø£ÙƒØ«Ø± Ø£Ù‡Ù…ÙŠØ©)
        mid_sentences.sort(key=len, reverse=True)
        summary_sentences.extend(mid_sentences[:num_sentences-1])
    
    return '. '.join(summary_sentences) + '.'

def extract_entities(text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙŠØ§Ù†Ø§Øª Ø¨Ø³ÙŠØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
    entities = []
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡ (ÙƒÙ„Ù…Ø§Øª ØªØ¨Ø¯Ø£ Ø¨Ø­Ø±Ù ÙƒØ¨ÙŠØ±)
    names = re.findall(r'\b[A-Z][a-z]+\b', text)
    for name in set(names):
        entities.append({'text': name, 'label': 'Ø´Ø®Øµ', 'start': text.find(name)})
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù…
    numbers = re.findall(r'\b\d+\b', text)
    for number in set(numbers):
        entities.append({'text': number, 'label': 'Ø±Ù‚Ù…', 'start': text.find(number)})
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙˆØ§Ø±ÙŠØ® Ø¨Ø³ÙŠØ·Ø©
    dates = re.findall(r'\b\d{4}\b|\b\d{1,2}/\d{1,2}/\d{4}\b', text)
    for date in set(dates):
        entities.append({'text': date, 'label': 'ØªØ§Ø±ÙŠØ®', 'start': text.find(date)})
    
    return entities

# Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
st.markdown('<h1 class="main-header">ğŸ“° Ù…Ø­Ù„Ù„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ</h1>', unsafe_allow_html=True)
st.markdown("---")

# ØªØ­Ø°ÙŠØ± Ø¨Ø³ÙŠØ·
st.info("ğŸ”§ **ÙˆØ¶Ø¹ Ø§Ù„ØªØ·ÙˆÙŠØ±**: ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø·Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§")

# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.title("ğŸ›ï¸ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…")
st.sidebar.markdown("---")

# Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„
analysis_type = st.sidebar.selectbox(
    "Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„",
    ["ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„", "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", "ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ", "ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ", "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"]
)

# Ø®ÙŠØ§Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
st.sidebar.markdown("### âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
show_stats = st.sidebar.checkbox("Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©", True)
show_word_freq = st.sidebar.checkbox("Ø¹Ø±Ø¶ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª", True)

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“Š Ø­ÙˆÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
st.sidebar.info(
    """
    **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**
    - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    - ØªØµÙ†ÙŠÙ Ù…Ø¨Ø³Ø·
    - ØªÙ„Ø®ÙŠØµ ØªÙ„Ù‚Ø§Ø¦ÙŠ
    - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
    - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
    
    **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ù‡Ø°Ø§ Ø¥ØµØ¯Ø§Ø± Ù…Ø¨Ø³Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    """
)

# Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
col1, col2 = st.columns([2, 1])

with col1:
    st.header("ğŸ“ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ")
    
    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
    user_text = st.text_area(
        "Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„:",
        height=300,
        placeholder="Ø§ÙƒØªØ¨ Ø£Ùˆ Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§..."
    )

with col2:
    st.header("ğŸ“Š Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©")
    
    if user_text:
        analysis = simple_text_analysis(user_text)
        
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", analysis['word_count'])
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù", analysis['char_count'])
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„", analysis['sentence_count'])
        
        # Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment_color = {
            'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': 'green',
            'Ø³Ù„Ø¨ÙŠ': 'red',
            'Ù…Ø­Ø§ÙŠØ¯': 'gray'
        }
        
        st.markdown(f"**Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:** <span style='color: {sentiment_color[analysis['sentiment']]}'>{analysis['sentiment']}</span>", 
                   unsafe_allow_html=True)

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
if user_text and st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„", type="primary"):
    
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
        analysis = simple_text_analysis(user_text)
        
        if analysis_type == "ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„":
            st.header("ğŸ” Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„")
            
            # ØªØ¨ÙˆÙŠØ¨Ø§Øª Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            tab1, tab2, tab3, tab4 = st.tabs(["Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "Ø§Ù„ØªØµÙ†ÙŠÙ", "Ø§Ù„ØªÙ„Ø®ÙŠØµ", "Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"])
            
            with tab1:
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª", analysis['word_count'])
                    st.metric("Ø§Ù„Ø¬Ù…Ù„", analysis['sentence_count'])
                
                with col2:
                    st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", analysis['positive_words'])
                    st.metric("Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ©", analysis['negative_words'])
                
                with col3:
                    st.markdown(f"**Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:** {analysis['category']}")
                    st.markdown(f"**Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:** {analysis['sentiment']}")
                
                if show_word_freq and analysis['most_common_words']:
                    st.subheader("Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹")
                    freq_df = pd.DataFrame(analysis['most_common_words'], columns=['Ø§Ù„ÙƒÙ„Ù…Ø©', 'Ø§Ù„ØªÙƒØ±Ø§Ø±'])
                    st.bar_chart(freq_df.set_index('Ø§Ù„ÙƒÙ„Ù…Ø©'))
            
            with tab2:
                st.subheader("ğŸ·ï¸ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ")
                st.success(f"**Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:** {analysis['category']}")
                confidence = analysis['category_confidence'] * 100
                st.progress(min(confidence, 100) / 100)
                st.write(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {confidence:.1f}%")
            
            with tab3:
                st.subheader("ğŸ“„ Ù…Ù„Ø®Øµ Ø§Ù„Ù†Øµ")
                summary = simple_summarize(user_text)
                st.write(summary)
                
                original_words = analysis['word_count']
                summary_words = len(summary.split())
                compression = (1 - summary_words/original_words) * 100 if original_words > 0 else 0
                
                st.info(f"ØªÙ… Ø¶ØºØ· Ø§Ù„Ù†Øµ Ù…Ù† {original_words} ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ {summary_words} ÙƒÙ„Ù…Ø© ({compression:.1f}% Ø¶ØºØ·)")
            
            with tab4:
                st.subheader("ğŸ” Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©")
                entities = extract_entities(user_text)
                
                if entities:
                    entities_df = pd.DataFrame(entities)
                    st.dataframe(entities_df)
                    
                    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
                    entity_counts = pd.Series([e['label'] for e in entities]).value_counts()
                    st.bar_chart(entity_counts)
                else:
                    st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙŠØ§Ù†Ø§Øª ÙˆØ§Ø¶Ø­Ø© ÙÙŠ Ø§Ù„Ù†Øµ")
        
        elif analysis_type == "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±":
            st.header("ğŸ˜Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")
            
            col1, col2 = st.columns(2)
            
            with col1:
                sentiment_emoji = {
                    'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': 'ğŸ˜Š',
                    'Ø³Ù„Ø¨ÙŠ': 'ğŸ˜',
                    'Ù…Ø­Ø§ÙŠØ¯': 'ğŸ˜'
                }
                
                st.markdown(f"## {sentiment_emoji[analysis['sentiment']]} {analysis['sentiment']}")
                
                st.metric("ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", analysis['positive_words'])
                st.metric("ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ©", analysis['negative_words'])
            
            with col2:
                # Ù…Ø®Ø·Ø· Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ø´Ø§Ø¹Ø±
                sentiment_data = pd.DataFrame({
                    'Ø§Ù„Ù†ÙˆØ¹': ['Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø³Ù„Ø¨ÙŠ'],
                    'Ø§Ù„Ø¹Ø¯Ø¯': [analysis['positive_words'], analysis['negative_words']]
                })
                st.bar_chart(sentiment_data.set_index('Ø§Ù„Ù†ÙˆØ¹'))
        
        elif analysis_type == "ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ":
            st.header("ğŸ·ï¸ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ")
            
            st.success(f"**Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:** {analysis['category']}")
            
            confidence = analysis['category_confidence'] * 100
            st.progress(min(confidence, 100) / 100)
            st.write(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {confidence:.1f}%")
        
        elif analysis_type == "ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ":
            st.header("ğŸ“„ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ")
            
            summary = simple_summarize(user_text)
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("### Ø§Ù„Ù…Ù„Ø®Øµ:")
                st.write(summary)
            
            with col2:
                original_words = analysis['word_count']
                summary_words = len(summary.split())
                compression = (1 - summary_words/original_words) * 100 if original_words > 0 else 0
                
                st.metric("Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ", f"{original_words} ÙƒÙ„Ù…Ø©")
                st.metric("Ø·ÙˆÙ„ Ø§Ù„Ù…Ù„Ø®Øµ", f"{summary_words} ÙƒÙ„Ù…Ø©")
                st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø¶ØºØ·", f"{compression:.1f}%")
        
        elif analysis_type == "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª":
            st.header("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª")
            
            entities = extract_entities(user_text)
            
            if entities:
                st.success(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(entities)} ÙƒÙŠØ§Ù†")
                
                entities_df = pd.DataFrame(entities)
                st.dataframe(entities_df)
                
                # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
                if len(entities) > 1:
                    entity_counts = pd.Series([e['label'] for e in entities]).value_counts()
                    st.subheader("ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª")
                    st.bar_chart(entity_counts)
            else:
                st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙŠØ§Ù†Ø§Øª ÙˆØ§Ø¶Ø­Ø© ÙÙŠ Ø§Ù„Ù†Øµ")

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
if user_text:
    st.markdown("---")
    st.markdown("### ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©")
    
    with st.expander("Ø¹Ø±Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„"):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            avg_word_length = sum(len(word) for word in user_text.split()) / len(user_text.split()) if user_text.split() else 0
            st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©", f"{avg_word_length:.1f}")
        
        with col2:
            avg_sentence_length = analysis['word_count'] / analysis['sentence_count'] if analysis['sentence_count'] > 0 else 0
            st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©", f"{avg_sentence_length:.1f}")
        
        with col3:
            unique_words = len(set(user_text.split()))
            diversity = unique_words / analysis['word_count'] * 100 if analysis['word_count'] > 0 else 0
            st.metric("ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª", f"{diversity:.1f}%")

st.markdown("---")
st.markdown("### ğŸ’¡ Ù†ØµØ§Ø¦Ø­")
with st.expander("Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ­Ù„ÙŠÙ„"):
    st.markdown("""
    - **Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:** ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØµØ­ÙŠØ­
    - **Ù„Ù„ØªØµÙ†ÙŠÙ:** Ø§Ø³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ ÙˆØ§Ø¶Ø­Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
    - **Ù„Ù„ØªÙ„Ø®ÙŠØµ:** Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø·ÙˆÙ„ ØªØ¹Ø·ÙŠ Ù…Ù„Ø®ØµØ§Øª Ø£ÙØ¶Ù„
    - **Ù„Ù„ÙƒÙŠØ§Ù†Ø§Øª:** Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ ÙˆØ£Ø±Ù‚Ø§Ù… ØªØ¹Ø·ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø£ÙˆØ¶Ø­
    - **Ù‡Ø°Ø§ Ø¥ØµØ¯Ø§Ø± ØªØ¬Ø±ÙŠØ¨ÙŠ:** Ø³ÙŠØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©
    """)

st.markdown("---")
st.markdown("**ğŸ“§ Ù„Ù„ØªÙˆØ§ØµÙ„ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ:** [Ø§ØªØµÙ„ Ø¨Ù†Ø§](mailto:support@example.com)")
